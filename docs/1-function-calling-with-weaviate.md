# Function Calling with Weaviate

## Introduction

The capabilities of Large Language Models (LLMs) are advancing quickly largely thanks to their integration in Compound AI Systems. From Zaharia et al., a Compound AI System "tackles AI tasks using multiple interacting components, including multiple calls to models, retrievers, or external tools" [2]. Connecting AI models to external tools complements their weaknesses, such as access to continually updating data or symbolic computation. However, we are now faced with a new research question: Are LLMs capable of formatting the correct arguments for a tool call? In order to answer this question, Patil et al. introduced the Berkeley Function Calling Leaderboard and the Gorilla LLM [3]. In this work, we test an adaptation of the Gorilla LLM training and evaluation algorithms to target search-enabled database APIs.

Most examples of LLMs and database access as a tool follow an architecture inspired by Retrieval-Augmented Generation (RAG) [4]. This describes a retrieve-augment-generate pipeline that conventionally entails first sending the prompt as the search query to retrieve from a search index. The search results are then passed to an LLM to answer a question, or complete tasks more broadly. In our work, we expand the capabilities of standard RAG systems to navigate multiple data collections and utilize filters, result aggregations, and groupby operations, in addition to search queries.

We compare the gpt-4o and gpt-4o-mini LLMs on the task of choosing the correct database query API given a hypothetical information need as input. Some queries only require a single API, such as "How many unique menu items are there in the restaurant menus that are priced under $20?". This can be answered with an integer filter that sets the price less than $20. Contrastively, the user query: "What is the average price of seasonal specialty menu items under $20, grouped by whether they are vegetarian or not?" requires routing the query to the Menu collection, formatting a search query for "seasonal specialties", setting a price filter of less than $20, calculating the average price of the results, and grouping them by the isVegetarian boolean property.

## Methodology

The Weaviate tool schema is implemented differently for OpenAI and Anthropic through dedicated builder functions. The process begins by calling `get_collections_info()` which connects to a Weaviate instance to retrieve metadata about available collections and their properties. This returns both a formatted description string detailing the collections and their properties, as well as a list of collection names that can be used as an enumeration. Due to token limits in function calling APIs, the description is carefully constructed to fit within 1024 tokens while maintaining all essential information about the schema.

The builder functions then construct provider-specific tool schemas - `OpenAITool` or `AnthropicTool` - each following their provider's function calling format while maintaining consistent query capabilities. The tool schemas expose a "query_database" function with parameters tailored to Weaviate's querying capabilities. The required parameter is "collection_name", which is restricted to the enumerated list of available collections. Optional parameters enable semantic search via "search_query", filtering through structured filter objects, and result aggregation using aggregation objects.

The tool schemas use structured Pydantic models to represent filters and aggregations, providing strong type validation and clear parameter boundaries. For filtering, the schema supports IntPropertyFilter, TextPropertyFilter, and BooleanPropertyFilter models. Numeric filters must specify a property name, comparison operator (=, <, >, <=, >=), and numeric value. Text filters support equality and LIKE operators, while boolean filters handle true/false comparisons. For aggregations, the schema uses IntAggregation, TextAggregation, and BooleanAggregation classes that enforce valid statistical operations for each data type - such as MIN, MAX, MEAN for numbers or TOP_OCCURRENCES for text. Additionally, grouping operations are supported through a GroupBy model that specifies the property to group results by.

This structured approach ensures that LLMs can only construct valid Weaviate queries by providing awareness of available collections and their properties, comprehensive querying features through strongly-typed models, and built-in validation of all query parameters. The implementation has been thoroughly tested with both GPT-4 and Claude models, demonstrating robust performance in translating natural language queries into valid Weaviate operations.

## Future

This work opens up several promising avenues for future research and development in the integration of LLMs with database systems. One key direction is the implementation of local deployment options through Ollama integration, which would address important concerns around latency, costs, and privacy while potentially leveraging specialized models for database operations. The system could also be enhanced through investigations into query generation approaches - comparing DSL string generation versus structured model objects - and expanding capabilities to handle multi-step queries and cross-collection joins.

Further improvements could come from schema-aware fine-tuning and query optimization strategies. Creating specialized models trained on specific database schemas could improve accuracy while reducing model size and costs, while incorporating performance metrics could help models generate not just correct but efficient queries. Additionally, expanding beyond Weaviate to support multiple database systems could create a unified natural language interface across different database backends. These developments would significantly advance the practical application of natural language interfaces to databases, making them more powerful and accessible for real-world use cases.